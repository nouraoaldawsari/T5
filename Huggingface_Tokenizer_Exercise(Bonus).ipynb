{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nouraoaldawsari/T5/blob/main/Huggingface_Tokenizer_Exercise(Bonus).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec47c97d",
      "metadata": {
        "id": "ec47c97d"
      },
      "source": [
        "# Exercise: Working with Hugging Face Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7668610a",
      "metadata": {
        "id": "7668610a"
      },
      "source": [
        "### Part 1: Basic Tokenization with Hugging Face Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fd04b75",
      "metadata": {
        "id": "5fd04b75"
      },
      "source": [
        "\n",
        "1. Install the Hugging Face `transformers` library:\n",
        "   ```bash\n",
        "   !pip install transformers\n",
        "   ```\n",
        "\n",
        "2. Choose a pre-trained tokenizer from Hugging Face’s model hub (e.g., `bert-base-uncased`, `gpt2`, etc.) and tokenize a piece of text:\n",
        "   \n",
        "   **Task**: Load the tokenizer and tokenize the sentence: `\"T5 is the greatest data science boot-camp!\"`\n",
        "\n",
        "   Below is a code block where you can perform this task:\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dffd763",
      "metadata": {
        "id": "9dffd763",
        "outputId": "4a9b3475-88cf-4d53-95e3-1f9c3612054c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['t', '##5', 'is', 'the', 'greatest', 'data', 'science', 'boot', '-', 'camp', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize the sentence\n",
        "sentence = \"T5 is the greatest data science boot-camp!\"\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "# Print the tokens\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27c9634",
      "metadata": {
        "id": "e27c9634"
      },
      "source": [
        "\n",
        "### Part 2: Encoding and Decoding\n",
        "\n",
        "3. Use the same tokenizer to encode the sentence (convert to token IDs) and then decode it back to text.\n",
        "\n",
        "   **Task**: Encode the sentence and then decode it back to text.\n",
        "\n",
        "   Below is a code block where you can perform this task:\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1292c1",
      "metadata": {
        "id": "9e1292c1",
        "outputId": "4db419b0-f577-4297-b4a6-2fb46f1c5162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1056, 2629, 2003, 1996, 4602, 2951, 2671, 9573, 1011, 3409, 999, 102]\n",
            "[CLS] t5 is the greatest data science boot - camp! [SEP]\n"
          ]
        }
      ],
      "source": [
        "# Encode the sentence\n",
        "input_ids = tokenizer.encode(sentence)\n",
        "\n",
        "# Print the encoded IDs\n",
        "print(input_ids)\n",
        "\n",
        "# Decode the encoded IDs back to text\n",
        "decoded_sentence = tokenizer.decode(input_ids)\n",
        "\n",
        "# Print the decoded sentence\n",
        "print(decoded_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a8df7c",
      "metadata": {
        "id": "14a8df7c"
      },
      "source": [
        "\n",
        "### Bonus Challenge\n",
        "\n",
        "4. **Custom Tokenizer**: Use Hugging Face’s `tokenizers` library to train a custom tokenizer on a dataset.\n",
        "   \n",
        "   You are provided with a dataset containing multiple sentences. Train a custom tokenizer using these sentences.\n",
        "\n",
        "   Below is a code block to train the tokenizer:\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30701893",
      "metadata": {
        "id": "30701893",
        "outputId": "0a0be1a3-1f65-4240-b8a8-7d78007f1da2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Sample dataset\n",
        "sentences = [\n",
        "    \"This is a sample sentence.\",\n",
        "    \"Another sentence for training.\",\n",
        "    \"Let's train a custom tokenizer!\"\n",
        "]\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train the tokenizer\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer.train_from_iterator(sentences, trainer)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save(\"custom_tokenizer.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b10db58",
      "metadata": {
        "id": "6b10db58",
        "outputId": "6064d108-1488-4923-d784-5e729641d82c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Transformers are amazing for NLP tasks.', 'Tokenization is essential for language models.', 'Byte Pair Encoding is a great subword tokenization algorithm.', 'Hugging Face makes it easy to work with pre-trained models.', 'Data science is the key to unlocking insights from data.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Provided dataset for the bonus challenge\n",
        "dataset = [\n",
        "    \"Transformers are amazing for NLP tasks.\",\n",
        "    \"Tokenization is essential for language models.\",\n",
        "    \"Byte Pair Encoding is a great subword tokenization algorithm.\",\n",
        "    \"Hugging Face makes it easy to work with pre-trained models.\",\n",
        "    \"Data science is the key to unlocking insights from data.\"\n",
        "]\n",
        "print(dataset)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}